[
  {
    "objectID": "proposal.html",
    "href": "proposal.html",
    "title": "Detecting Fake News with TF-IDF and Machine Learning",
    "section": "",
    "text": "import numpy as np\nimport pandas as pd"
  },
  {
    "objectID": "proposal.html#goal",
    "href": "proposal.html#goal",
    "title": "Detecting Fake News with TF-IDF and Machine Learning",
    "section": "Goal",
    "text": "Goal\nMy goal is to develop a transparent and accurate machine learning model that can classify news articles as fake or real based on their textual content and identify the words or phrases most predictive of fake news."
  },
  {
    "objectID": "proposal.html#dataset",
    "href": "proposal.html#dataset",
    "title": "Detecting Fake News with TF-IDF and Machine Learning",
    "section": "Dataset",
    "text": "Dataset\n\nfake_real_news = pd.read_csv(\"data/fake_or_real_news_data.csv\")\n\n# Rename unnamed column\nfake_real_news.rename(columns={\"Unnamed: 0\": \"article #\"}, inplace=True)\n\n# Print column names\ndisplay(fake_real_news.columns)\n\n# Print data types & shape of data\ndisplay(fake_real_news.info())\n\nIndex(['article #', 'title', 'text', 'label'], dtype='object')\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 6335 entries, 0 to 6334\nData columns (total 4 columns):\n #   Column     Non-Null Count  Dtype \n---  ------     --------------  ----- \n 0   article #  6335 non-null   int64 \n 1   title      6335 non-null   object\n 2   text       6335 non-null   object\n 3   label      6335 non-null   object\ndtypes: int64(1), object(3)\nmemory usage: 198.1+ KB\n\n\nNone\n\n\nThe dataset I am using for the project is the Fake and Real News Dataset from Kaggle. It consists of roughly 6,300 rows with four columns representing article title, text content, and label (fake or real). I will combine the title and text for a comprehensive representation; this dataset is appropriate for the project because it provides balanced and diverse examples of fake and real news. I chose this dataset because it includes the full details of articles and it’s also large enough to train the model and make predictions."
  },
  {
    "objectID": "proposal.html#questions",
    "href": "proposal.html#questions",
    "title": "Detecting Fake News with TF-IDF and Machine Learning",
    "section": "Questions",
    "text": "Questions\nCan we build an effective and interpretable fake news classifier using TF-IDF and machine learning?\nCan we identify which words or phrases are most predictive of fake news content using categorical features?\n\nfrom textblob import TextBlob\n\n# Function to get sentiment scores\ndef get_sentiment(text):\n    blob = TextBlob(str(text))  # Ensure it's a string\n    return pd.Series([blob.sentiment.polarity, \n                      blob.sentiment.subjectivity])\n\n# Title sentiment features\nfake_real_news[['title_polarity', \n                'title_subjectivity']] = fake_real_news['title'].apply(get_sentiment)\n\n# Text sentiment features\nfake_real_news[['text_polarity', \n                'text_subjectivity']] = fake_real_news['text'].apply(get_sentiment)\n\n# Preview the new dataset with sentiment features\nfake_real_news.head()\n\n\n\n\n\n\n\n\narticle #\ntitle\ntext\nlabel\ntitle_polarity\ntitle_subjectivity\ntext_polarity\ntext_subjectivity\n\n\n\n\n0\n8476\nYou Can Smell Hillary’s Fear\nDaniel Greenfield, a Shillman Journalism Fello...\nFAKE\n0.000000\n0.000000\n0.059595\n0.562654\n\n\n1\n10294\nWatch The Exact Moment Paul Ryan Committed Pol...\nGoogle Pinterest Digg Linkedin Reddit Stumbleu...\nFAKE\n0.125000\n0.175000\n0.082652\n0.518638\n\n\n2\n3608\nKerry to go to Paris in gesture of sympathy\nU.S. Secretary of State John F. Kerry said Mon...\nREAL\n0.000000\n0.000000\n0.102574\n0.348775\n\n\n3\n10142\nBernie supporters on Twitter erupt in anger ag...\n— Kaydee King (@KaydeeKing) November 9, 2016 T...\nFAKE\n-0.875000\n0.200000\n0.063645\n0.503563\n\n\n4\n875\nThe Battle of New York: Why This Primary Matters\nIt's primary day in New York and front-runners...\nREAL\n0.268182\n0.477273\n0.251709\n0.420109"
  },
  {
    "objectID": "proposal.html#feature-engineering",
    "href": "proposal.html#feature-engineering",
    "title": "Detecting Fake News with TF-IDF and Machine Learning",
    "section": "Feature Engineering",
    "text": "Feature Engineering\n\n\n\n\n\n\n\nFeature\nDescription\n\n\n\n\ntitle_polarity\nSentiment score of the title from -1 (negative) to +1 (positive)\n\n\ntitle_subjectivity\nSubjectivity score of the title from 0 (objective) to 1 (subjective)\n\n\ntext_polarity\nSentiment score of the full text from -1 (negative) to +1 (positive)\n\n\ntext_subjectivity\nSubjectivity score of the full text from 0 (objective) to 1 (subjective)"
  },
  {
    "objectID": "proposal.html#analysis-plan",
    "href": "proposal.html#analysis-plan",
    "title": "Detecting Fake News with TF-IDF and Machine Learning",
    "section": "Analysis plan",
    "text": "Analysis plan\n\nFirst, I will preprocess the data and apply TF-IDF to extract relevant textual features.\nNext, I will train machine learning classifiers and evaluate their performance to assess effectiveness.\nFinally, I will interpret the model to identify which words or patterns most strongly predict fake news."
  },
  {
    "objectID": "proposal.html#weekly-plan",
    "href": "proposal.html#weekly-plan",
    "title": "Detecting Fake News with TF-IDF and Machine Learning",
    "section": "Weekly Plan",
    "text": "Weekly Plan\n\n\n\n\n\n\n\nWeek\nTask\n\n\n\n\n1\nData loading, cleaning, preprocessing, and exploratory data analysis. Implement TF-IDF vectorization and basic logistic regression model.\n\n\n2\nModel training, hyperparameter tuning, and evaluation. Extract top predictive words/phrases using model coefficients. Begin documentation of results.\n\n\n3\nPrepare final analysis and visualizations of predictive words. Write up project report, finalize README files, and organize code repository structure."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Balancing Accuracy and Interpretability in Fake News Detection: Logistic Regression vs. Random Forest",
    "section": "",
    "text": "Recently, fake news has been spreading a lot globally, which really questions the credibility and trustworthiness of the news source. In order to stop falling for the fake news, we must do our own research by checking the news source and making sure it is credible; in addition, we should also consider verifying other news platforms are saying since this can mislead the viewers. This project builds a machine learning system that detects fake news using TF-IDF (Term Frequency Inverse Document Frequency) text features and well-known classification models. The approach is designed to be interpretable, so we not only predict whether an article is fake or real, but also identify the words and phrases most linked to fake content. Using a real-world dataset, we compare several models and show that interpretable methods can achieve strong accuracy while giving clear explanations for their predictions. This is useful for readers and viewers who want to understand why a news story may be unreliable."
  },
  {
    "objectID": "index.html#abstract",
    "href": "index.html#abstract",
    "title": "Balancing Accuracy and Interpretability in Fake News Detection: Logistic Regression vs. Random Forest",
    "section": "",
    "text": "Recently, fake news has been spreading a lot globally, which really questions the credibility and trustworthiness of the news source. In order to stop falling for the fake news, we must do our own research by checking the news source and making sure it is credible; in addition, we should also consider verifying other news platforms are saying since this can mislead the viewers. This project builds a machine learning system that detects fake news using TF-IDF (Term Frequency Inverse Document Frequency) text features and well-known classification models. The approach is designed to be interpretable, so we not only predict whether an article is fake or real, but also identify the words and phrases most linked to fake content. Using a real-world dataset, we compare several models and show that interpretable methods can achieve strong accuracy while giving clear explanations for their predictions. This is useful for readers and viewers who want to understand why a news story may be unreliable."
  },
  {
    "objectID": "index.html#introduction",
    "href": "index.html#introduction",
    "title": "Balancing Accuracy and Interpretability in Fake News Detection: Logistic Regression vs. Random Forest",
    "section": "Introduction",
    "text": "Introduction\nThe spread of fake news profoundly impacts the credibility and trustworthiness of news sources since news platforms and media have the tendency to exaggerate information just to get more public attention. A promising approach to predict the spread of fake news is to focus and analyze the language being used in the news since certain words and phrases often serve as strong indicators of whether the news is fake or real. In this project, I am using a dataset wherein each row consists of individual news articles, article title, and label (i.e., fake or real). Additional columns representing sentiment analysis were created using feature engineering. The actual names of the variables in the dataset are ‘title’, ‘text’, ‘label’, ‘title_polarity’, ‘title_subjectivity’, ‘text_polarity’, and ‘text_subjectivity’.\nBy applying TF-IDF feature extraction and machine learning methods, the goal is to not only classify news articles accurately, but also reveal patterns most predictive of fake news.\nIn this project, I aim to answer the fundamental questions:\nCan we build an effective and interpretable fake news classifier using TF-IDF and machine learning? Can we identify which words or phrases are most predictive of fake news content using categorical features?"
  },
  {
    "objectID": "index.html#model-mining-method",
    "href": "index.html#model-mining-method",
    "title": "Balancing Accuracy and Interpretability in Fake News Detection: Logistic Regression vs. Random Forest",
    "section": "Model & Mining Method",
    "text": "Model & Mining Method\nWe model the interaction between the features and words/phrases that represent fake news content. The dataset consists of article titles corresponding to their respective content which are classified as either fake or real.\nTo identify the phrases and words that are representative of fake news, I first encoded the ‘label’ column (i.e., classifier) into binary variables (i.e., 0 for REAL, 1 for FAKE) and applied feature engineering techniques thereby devising sentiment-based features including title polarity, title subjectivity, text polarity, and text subjectivity.\nFor preprocessing, I applied data cleaning methods by dropping irrelevant columns and splitting the model into a training and testing set to ensure proper model validation. Subsequently, I applied TF-IDF vectorization to transform textual content which emphasizes important words while reducing the impact of very common ones. Eventually, these features were combined with sentiment-based features to create a more informative set of predictors that reflect both word usage and emotional tone.\nIn addition, both logistic regression and random forest were applied to this representation to compare predictive power. Logistic regression was chosen for its interpretability, while random forest was selected for its ability to capture more complex, non-linear relationships. Model performance was tested and visualized using classification metrics such as accuracy, precision, recall, F1-score, and ROC AUC score (i.e., predictive power) along with a confusion matrix, and sideways bar plot."
  },
  {
    "objectID": "index.html#justification-of-approach",
    "href": "index.html#justification-of-approach",
    "title": "Balancing Accuracy and Interpretability in Fake News Detection: Logistic Regression vs. Random Forest",
    "section": "Justification of Approach",
    "text": "Justification of Approach\nThis approach is motivated by both the importance of accuracy and interpretability in fake news detection. Word-based models like TF-IDF are effective with their ability to identify distinct words and phrases, but they miss out on rhetorical and emotional patterns. Incorporating sentiment features addresses this limitation by providing emotions and subjectivity, which are often present in misleading or sensationalized content. Fake news frequently relies on emotionally charged language to attract attention, so combining linguistic and sentiment cues makes the classifier more robust.\nThe use of logistic regression provides a clear baseline that allows straightforward interpretation of feature weights, directly linking words or phrases to their predictive role. This makes it easier to understand what signals the model relies on when labeling news as fake or real. In contrast, the random forest classifier adds robustness by modeling non-linear interactions between features and reducing the risk of overfitting through ensemble averaging. It also provides feature importance scores, which help identify not only single keywords, but also combinations of features that contribute to classification.\nBy combining text mining (TF-IDF) with sentiment analysis, the methodology balances interpretability and predictive power while also highlighting words, phrases, and emotional signals that are most indicative of fake news. This combined approach ensures that the model is not only accurate, but it also provides insights that can guide further research into linguistic and psychological aspects of misinformation."
  },
  {
    "objectID": "index.html#code",
    "href": "index.html#code",
    "title": "Balancing Accuracy and Interpretability in Fake News Detection: Logistic Regression vs. Random Forest",
    "section": "Code",
    "text": "Code\n\nImport Libraries and Modules\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import roc_auc_score, roc_curve\nfrom sklearn.metrics import classification_report, confusion_matrix\nfrom textblob import TextBlob\nfrom scipy.sparse import hstack\nimport re\n\n\n\nData Cleaning\n\n# Load the data\nfake_real_news = pd.read_csv(\"data/fake_or_real_news_data.csv\")\n\n# Drop unnecessary column(s)\nfake_real_news_clean = fake_real_news.drop(\"Unnamed: 0\", axis=1)\n\n# Print column names\ndisplay(fake_real_news_clean.columns)\n\n# Print data types & shape of dataframe\ndisplay(fake_real_news_clean.info())\n\n# Check for missing values\ndisplay(fake_real_news_clean.isna().sum())\n\n# Display first few rows\ndisplay(fake_real_news_clean.head())\n\nIndex(['title', 'text', 'label'], dtype='object')\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 6335 entries, 0 to 6334\nData columns (total 3 columns):\n #   Column  Non-Null Count  Dtype \n---  ------  --------------  ----- \n 0   title   6335 non-null   object\n 1   text    6335 non-null   object\n 2   label   6335 non-null   object\ndtypes: object(3)\nmemory usage: 148.6+ KB\n\n\nNone\n\n\ntitle    0\ntext     0\nlabel    0\ndtype: int64\n\n\n\n\n\n\n\n\n\ntitle\ntext\nlabel\n\n\n\n\n0\nYou Can Smell Hillary’s Fear\nDaniel Greenfield, a Shillman Journalism Fello...\nFAKE\n\n\n1\nWatch The Exact Moment Paul Ryan Committed Pol...\nGoogle Pinterest Digg Linkedin Reddit Stumbleu...\nFAKE\n\n\n2\nKerry to go to Paris in gesture of sympathy\nU.S. Secretary of State John F. Kerry said Mon...\nREAL\n\n\n3\nBernie supporters on Twitter erupt in anger ag...\n— Kaydee King (@KaydeeKing) November 9, 2016 T...\nFAKE\n\n\n4\nThe Battle of New York: Why This Primary Matters\nIt's primary day in New York and front-runners...\nREAL\n\n\n\n\n\n\n\n\n\nConfusion Matrix (Random Forest)\n\n# Clean data\nfake_real_news.dropna(subset=['text'], inplace=True)\nfake_real_news['is_fake'] = fake_real_news['label']  # 1 = fake, 0 = real\n\n# Combine title and text\nfake_real_news['full_text'] = fake_real_news['title'].fillna('') + ' ' + fake_real_news['text'].fillna('')\n\n# Sentiment features\nfake_real_news['title_polarity'] = fake_real_news['title'].fillna('').apply(lambda x: TextBlob(x).sentiment.polarity)\nfake_real_news['title_subjectivity'] = fake_real_news['title'].fillna('').apply(lambda x: TextBlob(x).sentiment.subjectivity)\nfake_real_news['text_polarity'] = fake_real_news['text'].fillna('').apply(lambda x: TextBlob(x).sentiment.polarity)\nfake_real_news['text_subjectivity'] = fake_real_news['text'].fillna('').apply(lambda x: TextBlob(x).sentiment.subjectivity)\n\n# Split data into training & testing set\nX_train_text, X_test_text, y_train, y_test, train_sentiment, test_sentiment = train_test_split(\n    fake_real_news['full_text'],\n    fake_real_news['is_fake'],\n    fake_real_news[['title_polarity', 'title_subjectivity', 'text_polarity', 'text_subjectivity']],\n    test_size=0.2,\n    random_state=42\n)\n\n# TF-IDF vectorization \ntf_idf = TfidfVectorizer(\n    ngram_range=(2,2), \n    stop_words='english',\n    min_df=3,\n    max_df=0.95,\n    max_features=20000,\n    token_pattern=r'(?u)\\b[a-zA-Z][a-zA-Z]+\\b',\n    sublinear_tf=True\n)\n\nX_train_tfidf = tf_idf.fit_transform(X_train_text)\nX_test_tfidf = tf_idf.transform(X_test_text)\n\n# Combine TF-IDF and sentiment features\nX_train_combined = hstack([X_train_tfidf, train_sentiment])\nX_test_combined = hstack([X_test_tfidf, test_sentiment])\n\n# Train model using random forest\nrf = RandomForestClassifier(n_estimators=200, random_state=42, n_jobs=-1)\nrf.fit(X_train_combined, y_train)\n\n# Predictions & evaluation\ny_pred = rf.predict(X_test_combined)\nprint(classification_report(y_test, y_pred))\n\n# Probabilities for positive class\ny_proba = rf.predict_proba(X_test_combined)[:, 1]\n\n# ROC AUC score\nroc_auc = roc_auc_score(y_test, y_proba)\nprint(f\"ROC AUC Score \\033[91m(Random Forest)\\033[0m: {roc_auc:.4f}\")\n\n# Display confusion matrix\ncm = confusion_matrix(y_test, y_pred)\nsns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n            xticklabels=[\"Real\", \"Fake\"],\n            yticklabels=[\"Real\", \"Fake\"])\nplt.xlabel(\"Predicted\")\nplt.ylabel(\"Actual\")\nplt.title(\"Fake News Confusion Matrix\")\nplt.show()\n\n# Get feature names directly from the vectorizer used to fit X_train_tfidf\nfeature_names = tf_idf.get_feature_names_out()\n\n# Feature importance (phrases & sentiment)\nimportances = rf.feature_importances_\n\nmin_len = min(len(feature_names), len(importances))\nfeature_names = feature_names[:min_len]\nimportances = importances[:min_len]\n\n# Put into DataFrame\nfeat_df = pd.DataFrame({\n    \"feature\": feature_names,\n    \"importance\": importances\n})\n\n# Sort by importance\n# Select top 20\nfeat_df = feat_df.sort_values(\"importance\", ascending=False).head(20)\n\n\n              precision    recall  f1-score   support\n\n\n\n        FAKE       0.88      0.92      0.90       628\n\n        REAL       0.92      0.88      0.90       639\n\n\n\n    accuracy                           0.90      1267\n\n   macro avg       0.90      0.90      0.90      1267\n\nweighted avg       0.90      0.90      0.90      1267\n\n\n\nROC AUC Score (Random Forest): 0.9595\n\n\n\n\n\n\n\n\n\n\n\n\n\nSideways Bar Plot\n\n\nDetermine whether top 20 phrases are REAL or FAKE\n\n# Train on phrases only\n\n# Make a copy to secure original data\ndf = fake_real_news_clean.copy()\ndf = df.dropna(subset=['text'])\ndf['full_text'] = df['title'].fillna('') + ' ' + df['text'].fillna('')\ndf['label_bin'] = df['label'].map({'FAKE': 1, 'REAL': 0})\n\n# Year-removal\ndef clean_text(t): \n    return re.sub(r'\\b\\d{4}\\b', '', t)\n\nX = df['full_text'].apply(clean_text)\ny = df['label_bin']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n\n# Get feature names\ntf_idf = TfidfVectorizer(\n    ngram_range=(2,3),     # phrases only\n    stop_words='english',  # keep stop-words so phrases like \"new york\" remain intact\n    min_df=3,              # lower so phrases aren’t filtered out\n    max_df=0.95,\n    max_features=20000,\n    token_pattern=r'(?u)\\b[a-zA-Z][a-zA-Z]+\\b', # words with letters only\n    sublinear_tf=True\n)\n\nX_train_tfidf = tf_idf.fit_transform(X_train)\nX_test_tfidf = tf_idf.transform(X_test)\n\nfeature_names = tf_idf.get_feature_names_out()\n\n# Feature importances from RF\nimportances = rf.feature_importances_\n\nmin_len = min(len(feature_names), len(importances))\nfeature_names = feature_names[:min_len]\nimportances = importances[:min_len]\n\n# Build DataFrame\nfeat_df = pd.DataFrame({\n    \"feature\": feature_names,\n    \"importance\": importances\n})\n\n# Split training matrix by label\nfake_mask = (y_train == 1).values\nreal_mask = (y_train == 0).values\n\n# Compute average TF-IDF for each phrase in fake vs real docs\nfake_avg = np.asarray(X_train_tfidf[fake_mask].mean(axis=0)).ravel()\nreal_avg = np.asarray(X_train_tfidf[real_mask].mean(axis=0)).ravel()\n\n# Add to DataFrame\nfeat_df[\"fake_avg\"] = fake_avg[:min_len]\nfeat_df[\"real_avg\"] = real_avg[:min_len]\n\n# Determine whether phrase sounds real or fake\nfeat_df[\"leans_fake\"] = feat_df[\"fake_avg\"] &gt; feat_df[\"real_avg\"]\nfeat_df[\"class_assoc\"] = feat_df[\"leans_fake\"].map({True: \"FAKE\", False: \"REAL\"})\n\n# Sort by importance \n# Select top 20\ntop20 = feat_df.sort_values(\"importance\", ascending=False).head(20)\n\n# Reset index so first column is 'rank'\ntop20_reset = top20.reset_index(drop=True).reset_index()\ntop20_reset.rename(columns={\"index\": \"rank\"}, inplace=True)\ntop20_reset[\"rank\"] += 1\n\nprint(top20_reset[[\"rank\", \"feature\", \"importance\", \"class_assoc\"]])\n\n# Bar plot with color coding\nplt.figure(figsize=(9,6))\nsns.barplot(\n    data=top20,\n    x=\"importance\",\n    y=\"feature\",\n    hue=\"class_assoc\",\n    dodge=False,\n    palette={\"FAKE\":\"red\", \"REAL\":\"green\"}\n)\nplt.title(\"Top 20 Important Features (with Fake/Real Lean)\")\nplt.xlabel(\"Importance\")\nplt.ylabel(\"Phrase\")\nplt.show()\n\n    rank                 feature  importance class_assoc\n0      1   president elect trump    0.009712        FAKE\n1      2           time actually    0.009594        FAKE\n2      3         global economic    0.009287        REAL\n3      4        members american    0.008460        REAL\n4      5              wing party    0.007827        REAL\n5      6          just years ago    0.007483        REAL\n6      7           creating jobs    0.007385        FAKE\n7      8     predominantly black    0.007203        REAL\n8      9              big donors    0.007064        REAL\n9     10  trump general election    0.006726        REAL\n10    11             black white    0.006412        FAKE\n11    12           new political    0.005387        FAKE\n12    13            gop majority    0.005145        REAL\n13    14        background check    0.005137        REAL\n14    15            just example    0.004935        REAL\n15    16       immediately clear    0.004625        REAL\n16    17         november voting    0.004448        FAKE\n17    18          president want    0.004285        REAL\n18    19         restoration act    0.004235        REAL\n19    20            obama called    0.004054        REAL\n\n\n\n\n\n\n\n\n\n\n\nDetermine coefficients of phrases (Logistic Regression)\n\n# Train a Logistic Regression classifier\nlog_reg = LogisticRegression(max_iter=1000, class_weight=\"balanced\", C=10)\nlog_reg.fit(X_train_tfidf, y_train)\n\n# Get feature names\nfeature_names = tf_idf.get_feature_names_out()\n\n# Get coefficients (one coefficient per feature)\ncoefs = log_reg.coef_[0]\n\n# Build DataFrame for inspection\ncoef_df = pd.DataFrame({\n    \"feature\": feature_names,\n    \"coefficient\": coefs\n})\n\n# Sort by absolute value of coefficient (strongest predictors first)\ncoef_df = coef_df.reindex(coef_df.coefficient.abs().sort_values(ascending=False).index)\n\n# Display top 20 fake predictors (positive coefficients → FAKE)\ntop_fake = coef_df.head(20)\n\nprint(\"\\nTop 20 phrases most predictive of FAKE news (Logistic Regression):\")\nfor feat, coef in zip(top_fake['feature'], top_fake['coefficient']):\n    print(f\"{feat}: {coef:.4f}\")\n\n# Display bar plot\nplt.figure(figsize=(8,6))\nsns.barplot(x=\"coefficient\", y=\"feature\", data=top_fake, palette=\"coolwarm\")\nplt.title(\"Top 20 Phrases Predicting FAKE News (Logistic Regression)\")\nplt.xlabel(\"Coefficient (positive = FAKE, negative = REAL)\")\nplt.ylabel(\"Feature\")\nplt.show()\n\n\nTop 20 phrases most predictive of FAKE news (Logistic Regression):\nted cruz: -5.9980\nfox news: -5.8945\npresident obama: -5.8622\nhillary clinton: 5.4933\ntold cnn: -5.2391\nclinton said: -5.1027\nbernie sanders: -5.0025\njeb bush: -4.5464\nobama said: -4.4792\nassociated press: -4.4605\nmarco rubio: -4.3202\npolitical polarization: -4.2691\nmainstream media: 4.1390\nshort url: 4.1369\ncontributed report: -4.0943\ncharlie hebdo: -4.0236\nwhite house: -3.9440\njohn podesta: 3.9013\nplanned parenthood: -3.8648\nscott walker: -3.8459\n\n\n\n\n\n\n\n\n\n\n\nConfusion Matrix (Logistic Regression)\n\ndf.dropna(subset=['text'], inplace=True)\ndf['is_fake'] = df['label'] # 1 = fake, 0 = real\n\n# Combine title & text\ndf['full_text'] = df['title'].fillna('') + ' ' + df['text'].fillna('')\n\n# Train/test split\nX_train, X_test, y_train, y_test = train_test_split(\n    df['full_text'], \n    df['is_fake'], \n    test_size=0.2, \n    random_state=42\n)\n\n# TF-IDF vectorization\ntf_idf = TfidfVectorizer(ngram_range=(2,3), stop_words=\"english\", max_df=0.95, min_df=5, max_features=20000)\nX_train_vec = tf_idf.fit_transform(X_train)\nX_test_vec = tf_idf.transform(X_test)\n\n# Sentiment features\ndef get_sentiment(texts):\n    polarity = []\n    subjectivity = []\n    for t in texts:\n        blob = TextBlob(str(t))\n        polarity.append(blob.sentiment.polarity)\n        subjectivity.append(blob.sentiment.subjectivity)\n    return pd.DataFrame({\"polarity\": polarity, \"subjectivity\": subjectivity})\n\ntrain_sentiment = get_sentiment(X_train)\ntest_sentiment = get_sentiment(X_test)\n\n# Merge TF-IDF & sentiment\nX_train_final = hstack([X_train_vec, train_sentiment])\nX_test_final = hstack([X_test_vec, test_sentiment])\n\n# Train model using logistic regression\nclf = LogisticRegression(max_iter=1000)\nclf.fit(X_train_final, y_train)\n\n# Predict & evaluate\ny_pred = clf.predict(X_test_final)\ny_proba = clf.predict_proba(X_test_final)[:, 1]\n\nprint(classification_report(y_test, y_pred))\n\n# ROC AUC score\nroc_auc = roc_auc_score(y_test, y_proba)\nprint(f\"ROC AUC Score (Logistic Regression): {roc_auc:.4f}\")\n\n# Display confusion matrix\ncm = confusion_matrix(y_test, y_pred)\nsns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=[\"Real\", \"Fake\"], yticklabels=[\"Real\", \"Fake\"])\nplt.xlabel(\"Predicted\")\nplt.ylabel(\"Actual\")\nplt.title(\"Fake News Confusion Matrix (LogReg + Sentiment)\")\nplt.show()\n\n              precision    recall  f1-score   support\n\n        FAKE       0.90      0.93      0.91       628\n        REAL       0.93      0.90      0.91       639\n\n    accuracy                           0.91      1267\n   macro avg       0.91      0.91      0.91      1267\nweighted avg       0.91      0.91      0.91      1267\n\nROC AUC Score (Logistic Regression): 0.9760\n\n\n\n\n\n\n\n\n\n\n\nDetermine weights of phrases (Logistic Regression)\n\n# Define and fit vectorizer\ntf_idf = TfidfVectorizer(\n    ngram_range=(2,3),   # phrases\n    stop_words='english',\n    min_df=3,\n    max_df=0.95,\n    max_features=20000,\n    token_pattern=r'(?u)\\b[a-zA-Z][a-zA-Z]+\\b',\n    sublinear_tf=True\n)\n\nX_train_tfidf = tf_idf.fit_transform(X_train)\n\n# Train Logistic Regression classifier\nlog_reg = LogisticRegression(max_iter=1000, class_weight=\"balanced\")\nlog_reg.fit(X_train_tfidf, y_train)\n\nfeature_names = tf_idf.get_feature_names_out()\n\n# Get coefficients\ncoefs = log_reg.coef_[0]\n\n# Convert coefficients to weights (odds ratios)\nweights = np.exp(coefs)\n\n# Build DataFrame\nweights_df = pd.DataFrame({\n    \"feature\": feature_names,\n    \"coefficient\": coefs,\n    \"weight\": weights\n})\n\n# Sort by coefficient (positive ones are FAKE predictors)\nweights_df = weights_df.sort_values(\"coefficient\", ascending=False)\n\n# Show top 20 FAKE predictors\ntop_fake = weights_df.head(20)\n\nprint(\"\\nTop 20 phrases most predictive of FAKE news (Logistic Regression Weights):\")\nfor feat, coef, w in zip(top_fake['feature'], top_fake['coefficient'], top_fake['weight']):\n    print(f\"{feat}: coef={coef:.4f}, weight={w:.4f}\")\n\n# Display bar plot of weights\nplt.figure(figsize=(8,6))\nsns.barplot(x=\"weight\", y=\"feature\", data=top_fake, palette=\"coolwarm\")\nplt.title(\"Top 20 Phrases Predicting FAKE News (Logistic Regression Weights)\")\nplt.xlabel(\"Weight (Odds Ratio)\")\nplt.ylabel(\"Feature\")\nplt.show()\n\n\nTop 20 phrases most predictive of FAKE news (Logistic Regression Weights):\nfox news: coef=3.1439, weight=23.1937\nted cruz: coef=3.0552, weight=21.2255\npresident obama: coef=2.9680, weight=19.4526\nwhite house: coef=2.7165, weight=15.1277\nislamic state: coef=2.4860, weight=12.0127\ntold cnn: coef=2.4781, weight=11.9185\njeb bush: coef=2.3153, weight=10.1275\nmarco rubio: coef=2.2940, weight=9.9147\nnew hampshire: coef=2.1515, weight=8.5982\nbernie sanders: coef=2.1398, weight=8.4974\nassociated press: coef=2.0897, weight=8.0824\ntrump said: coef=2.0127, weight=7.4834\nclinton said: coef=1.9941, weight=7.3457\nsupreme court: coef=1.9886, weight=7.3055\ncontributed report: coef=1.9483, weight=7.0165\ngeneral election: coef=1.9424, weight=6.9751\nsouth carolina: coef=1.9272, weight=6.8705\nobama said: coef=1.9266, weight=6.8658\nobama administration: coef=1.7810, weight=5.9356\nscott walker: coef=1.7234, weight=5.6036"
  },
  {
    "objectID": "index.html#discussion-of-results",
    "href": "index.html#discussion-of-results",
    "title": "Balancing Accuracy and Interpretability in Fake News Detection: Logistic Regression vs. Random Forest",
    "section": "Discussion of Results",
    "text": "Discussion of Results\nIn this project, I used Logistic Regression to interpret feature importance by examining coefficients and identifying phrases most predictive of fake vs. real news. In parallel, I used a Random Forest classifier primarily for robust classification performance and complementary feature ranking. Together, these models provided both predictive accuracy and interpretability.\nThe bar plot using random forest classifier highlights the top 20 phrases that the Random Forest model relies on to distinguish fake news from real news, ranked by their importance in the model. It’s effectively a visual explanation of phrases the model pays attention to when making predictions, e.g., “president elect trump” being the most important means that this phrase was very helpful to the model in separating FAKE from REAL.\nAfter evaluating both logistic and random forest regression, the ROC AUC scores were 0.9760 and 0.9595, respectively. These results indicate that logistic regression can correctly distinguish between fake and real news 97.6% of the time, while random forest achieves a slightly lower performance of 95.95%. Such high scores suggest that both models are equally capable of performing the task, with minimal difference in their discriminative ability.\nThe confusion matrix further highlights the effectiveness of both models. For logistic regression, fake news predictions achieved a precision of 0.90, recall of 0.93, and F1-score of 0.91. Real news predictions showed a precision of 0.93, recall of 0.90, and F1-score of 0.91, with an overall accuracy of 91%. In contrast, the random forest model had a slightly lower performance than logistic regression with an overall accuracy of 90%, correctly classifying true positives and true negatives at a very similar frequency. This indicates that while random forest provides strong interpretability, logistic regression adds a small, but measurable improvement in predictive performance.\nIn addition, the two-way bar plot provided insights into which words and phrases are most indicative of fake versus real news. Words associated with fake news carried positive coefficients, while those linked to real news carried negative coefficients.\nHere, the coefficients represent how much each feature contributes in the log-odds space, while the weights calculated as exp(coefficient) show the multiplicative effect on the odds. For example, a coefficient of 0.7 means \\(\\exp(0.7)\\) ≈ 2, which doubles the odds of being fake. Weights greater than 1 increase the odds of fake news, while weights less than 1 suggest that a feature leans toward real news.\nThis project shows that TF-IDF with machine learning can build an accurate and interpretable fake news classifier. Logistic Regression and Random Forest both achieved high ROC AUC scores (&gt;0.95), confirming strong predictive ability. This project analysis also highlights the linguistic patterns that drive these predictions. Feature analysis highlighted the key phrases most indicative of fake versus real news, however, some predictive words showed discrepancies. This ultimately conveys that interpretability is not always perfectly reliable."
  },
  {
    "objectID": "presentation.html#background-information",
    "href": "presentation.html#background-information",
    "title": "Balancing Accuracy and Interpretability in Fake News Detection: Logistic Regression vs. Random Forest",
    "section": "Background Information",
    "text": "Background Information\n\nSpread of fake news profoundly impacts the credibility and trustworthiness of news sources.\n\nNews platforms and media have the tendency to exaggerate information just to get more public attention\nA promising approach - to focus and analyze the language being used in the news\n\nCertain words and phrases often serve as strong indicators of whether the news is fake or real\n\nIn this project, I am using a dataset wherein each row consists of individual news articles, article title, and label (i.e., fake or real)\nAdditional columns to represent sentiment analysis were created using feature engineering\n\nActual names of the variables in the dataset - ‘title’, ‘text’, ‘label’, ‘title_polarity’, ‘title_subjectivity’, ‘text_polarity’, and ‘text_subjectivity’.\n\n\n\nBy applying TF-IDF feature extraction and machine learning methods, the goal is to not only classify news articles accurately, but also reveal patterns most predictive of fake news."
  },
  {
    "objectID": "presentation.html#question",
    "href": "presentation.html#question",
    "title": "Balancing Accuracy and Interpretability in Fake News Detection: Logistic Regression vs. Random Forest",
    "section": "Question",
    "text": "Question\n\nCan we build an effective and interpretable fake news classifier using TF-IDF and machine learning?\nCan we identify which words or phrases are most predictive of fake news content using categorical features?"
  },
  {
    "objectID": "presentation.html#method-of-approach",
    "href": "presentation.html#method-of-approach",
    "title": "Balancing Accuracy and Interpretability in Fake News Detection: Logistic Regression vs. Random Forest",
    "section": "Method of Approach",
    "text": "Method of Approach\n\nCleaned data thereby dropping irrelevant columns\nFeature engineering techniques: generated columns representing sentiment analysis\n\nColumn names are ‘title’, ‘text’, ‘label’, ‘title_polarity’, ‘title_subjectivity’, ‘text_polarity’, and ’text_subjectivity.\n\nTrained model using logistic regression, random forest, and TF-IDF (Term Frequency Inverse Document Frequency)\nCompared models using ROC AUC, F1-score, recall, accuracy, and precision"
  },
  {
    "objectID": "presentation.html#column-definitions",
    "href": "presentation.html#column-definitions",
    "title": "Balancing Accuracy and Interpretability in Fake News Detection: Logistic Regression vs. Random Forest",
    "section": "Column Definitions",
    "text": "Column Definitions\n\nSome columns to describe from dataset:\n#: article number\ntitle: name of news article\ntext: contents of the article\nlabel: classifies the news as either FAKE or REAL\nHowever, I dropped the ‘#’ column since it’s irrelevant."
  },
  {
    "objectID": "presentation.html#dataset---first-few-rows",
    "href": "presentation.html#dataset---first-few-rows",
    "title": "Balancing Accuracy and Interpretability in Fake News Detection: Logistic Regression vs. Random Forest",
    "section": "Dataset - First Few Rows",
    "text": "Dataset - First Few Rows"
  },
  {
    "objectID": "presentation.html#bar-plot---top-20-features-random-forest",
    "href": "presentation.html#bar-plot---top-20-features-random-forest",
    "title": "Balancing Accuracy and Interpretability in Fake News Detection: Logistic Regression vs. Random Forest",
    "section": "Bar Plot - Top 20 Features (Random Forest)",
    "text": "Bar Plot - Top 20 Features (Random Forest)"
  },
  {
    "objectID": "presentation.html#bar-plot---top-20-features-interpretation",
    "href": "presentation.html#bar-plot---top-20-features-interpretation",
    "title": "Balancing Accuracy and Interpretability in Fake News Detection: Logistic Regression vs. Random Forest",
    "section": "Bar Plot - Top 20 Features (Interpretation)",
    "text": "Bar Plot - Top 20 Features (Interpretation)\n\nThe bar plot using random forest classifier highlights the top 20 phrases that the RF model relies on to distinguish fake news from real news, ranked by their importance.\nThe model pays most attention to the top 20 phrases when deciding if an article is fake or real.\n\ne.g., “president called trump” is the strongest signal feature in the trained model, while “obama called” is still useful, but weaker."
  },
  {
    "objectID": "presentation.html#confusion-matrix-random-forest",
    "href": "presentation.html#confusion-matrix-random-forest",
    "title": "Balancing Accuracy and Interpretability in Fake News Detection: Logistic Regression vs. Random Forest",
    "section": "Confusion Matrix (Random Forest)",
    "text": "Confusion Matrix (Random Forest)"
  },
  {
    "objectID": "presentation.html#bar-plot---coefficients-of-phrases-logistic-regression",
    "href": "presentation.html#bar-plot---coefficients-of-phrases-logistic-regression",
    "title": "Balancing Accuracy and Interpretability in Fake News Detection: Logistic Regression vs. Random Forest",
    "section": "Bar Plot - Coefficients of Phrases (Logistic Regression)",
    "text": "Bar Plot - Coefficients of Phrases (Logistic Regression)"
  },
  {
    "objectID": "presentation.html#confusion-matrix-logistic-regression",
    "href": "presentation.html#confusion-matrix-logistic-regression",
    "title": "Balancing Accuracy and Interpretability in Fake News Detection: Logistic Regression vs. Random Forest",
    "section": "Confusion Matrix (Logistic Regression)",
    "text": "Confusion Matrix (Logistic Regression)"
  },
  {
    "objectID": "presentation.html#bar-plot---weights-of-phrases-logistic-regression",
    "href": "presentation.html#bar-plot---weights-of-phrases-logistic-regression",
    "title": "Balancing Accuracy and Interpretability in Fake News Detection: Logistic Regression vs. Random Forest",
    "section": "Bar Plot - Weights of Phrases (Logistic Regression)",
    "text": "Bar Plot - Weights of Phrases (Logistic Regression)"
  },
  {
    "objectID": "presentation.html#coefficients-weights-plot-interpretation",
    "href": "presentation.html#coefficients-weights-plot-interpretation",
    "title": "Balancing Accuracy and Interpretability in Fake News Detection: Logistic Regression vs. Random Forest",
    "section": "Coefficients & Weights Plot (Interpretation)",
    "text": "Coefficients & Weights Plot (Interpretation)\n\nThe two-way bar plot provides insights into which words and phrases are most indicative of fake versus real news.\nWords associated with fake news carried positive coefficients, while those linked to real news carried negative coefficients.\n\nCoefficients: represent how much each feature contributes in the log-odds space\nWeights calculated as exp(coefficient) show the multiplicative effect on the odds."
  },
  {
    "objectID": "presentation.html#evaluating-logistic-random-forest-regression",
    "href": "presentation.html#evaluating-logistic-random-forest-regression",
    "title": "Balancing Accuracy and Interpretability in Fake News Detection: Logistic Regression vs. Random Forest",
    "section": "Evaluating Logistic & Random Forest Regression",
    "text": "Evaluating Logistic & Random Forest Regression\n\nROC AUC (LR) = 0.9760, ROC AUC (RF) = 0.9595\n\nLR can correctly distinguish between fake and real news 97.6% of the time\nRF achieves a slightly lower performance of 95.95%.\n\nThe confusion matrix further highlights the effectiveness of both models.\n\nBoth RF and LR model had a very similar performance in terms of their ability correctly classifying true positives and true negatives."
  },
  {
    "objectID": "presentation.html#key-takeaway",
    "href": "presentation.html#key-takeaway",
    "title": "Balancing Accuracy and Interpretability in Fake News Detection: Logistic Regression vs. Random Forest",
    "section": "Key Takeaway",
    "text": "Key Takeaway\n\nThis project shows that TF-IDF with machine learning can build an accurate and interpretable fake news classifier.\nLR and RF both achieved high ROC AUC scores (&gt;0.95), confirming strong predictive ability.\nThis project analysis also highlights the linguistic patterns that drive these predictions.\nFeature analysis highlighted key phrases most indicative of fake vs. real news.\n\nSome predictive words showed discrepancies, ultimately conveying that interpretability is not always perfectly reliable."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "This project was developed by [Team Name] For INFO 523 - Data Mining and Discovery at the University of Arizona, taught by Dr. Greg Chism. The team is comprised of the following team members.\n\nTeam member 1: One sentence description of Team member 1 (e.g., year, major, etc.).\nTeam member 2: One sentence description of Team member 2 (e.g., year, major, etc.).\nTeam member 3: One sentence description of Team member 3 (e.g., year, major, etc.).\nTeam member 4: One sentence description of Team member 4 (e.g., year, major, etc.)."
  }
]